#+TITLE: ComBau Summary
#+AUTHOR: Olivier Lischer
#+SETUPFILE: ../latex_includes.conf

#+begin_src latex
  \pagestyle{fancy}
  \fancyhf{}
  \fancyhead[R]{ComBau-HS22}
  \fancyhead[L]{Summary}
  \fancyfoot[CE,CO]{\leftmark}
  \fancyfoot[R]{\thepage}
  \fancyfoot[L]{Olivier Lischer}

  \tableofcontents
  \newpage
#+end_src

#+LATEX: \begin{multicols}{3}
* Compiler & Runtime System
** Runtime System
A Runtime System supports the application execution using software and hardware mechanism.
** Architecture

#+LATEX: {
[[file:img/cpp_architecutre.png]]
#+LATEX: \captionof{figure}{C++ Architecture}\label{fig:cpp-architecture}
#+LATEX: }


#+LATEX: {
[[file:img/java_csharp_architecture.png]]
#+LATEX: \captionof{figure}{Java / C# Architecture}\label{fig:java-csharp-architecture}
#+LATEX: }

#+LATEX: {
[[file:img/js_architecture.png]]
#+LATEX: \captionof{figure}{JS Architecture}\label{fig:js-architecture}
#+LATEX: }

** Compiler Architecture
A compiler consists of the following parts:
- Lexer 
- Parser
- Semantic Checker 
- Optimization
- Code Generator


#+LATEX: {
[[file:img/compiler_architecture.png]]
#+LATEX: \captionof{figure}{General Compiler Architecture}\label{fig:general-compiler-architecture}
#+LATEX: }

** Runtime Architecture
A Runtime System consists of the following parts:
- Loader
- Interpreter
- Metadata, Heap, Stack
- optional Garbage Collection
- optional Just-In-Time Compiler


#+LATEX: {
[[file:img/runtime_system_architecture.png]]
#+LATEX: \captionof{figure}{Components of a Runtime System}\label{fig:components-of-a-runtime-system}
#+LATEX: }

** EBNF
The syntax of a language can be expressed using the (E)BNF.
The EBNF has some convenient features but can not express more than the BNF.
** Syntax vs. Semantic
The syntax defines the structure of the program while the semantic describes the meaning behind the syntax.
The syntax is often described using the [[id:9e64bd75-d1fc-40ef-80d2-fa8d70bc041b][EBNF]].
The semantic is often described in prose.
* Lexer
** Lexer
A Lexer takes a string of chars as input (source code) and produces a stream of terminal symbols (tokens).
You want to use a lexer because it will help to parse the input.

#+LATEX: {
[[file:img/lexer_in_output.png]]
#+LATEX: \captionof{figure}{Input and output from a Lexer}\label{fig:input-and-output-from-a-lexer}
#+LATEX: }

A lexer does support only [[id:094b594b-304c-46f1-8eb5-bf27db2c7b77][Regular Language]].
That means, languages which can expressed using [[id:9e64bd75-d1fc-40ef-80d2-fa8d70bc041b][EBNF]] without recursion.
Therefore, a look ahead of one char is enough to implement a lexer.

#+begin_example
Integer = Digit { Digit }. // regular
Digit = "0" | ... | "9".

Ausdruck = [ "(" Ausdruck ")" ]. // not regular
#+end_example
** Chomsky Hierarchy
The Chomsky Hierarchy describes what is required to read and understand a specific language.
Lexer, Parser and Semantic Checker reads the Language while the [[id:60b0e92e-4c0c-469d-a899-ec8a4c7a3527][DEA]], Push down Automata and Bounded Turing Machine runs the application.

#+LATEX: {
[[file:img/chomsky_hierarchie.png]]
#+LATEX: \captionof{figure}{Chomsky Hierarchy}\label{fig:chomsky-hierarchy}
#+LATEX: }

* Parser
** Parser
A Parser takes the output of a [[id:2a6bbd33-de6c-4b7f-9f2e-69a8d850c225][Lexer]] (token stream) and checks if the input follows the given syntax.
A parser can read only context free languages (see [[*Chomsky Hierarchy]]).

The output from a parser is normally an [[id:9d516b01-0fd4-416d-9bb8-8decd793fc82][Abstract Syntax Tree]] or [[id:fc66c6d6-33a4-4e4a-9d73-8e44e19f0516][Concrete Syntax Tree]].
If you write the parser by your self you normally want to create an [[id:9d516b01-0fd4-416d-9bb8-8decd793fc82][AST]].

#+LATEX: {
[[file:img/parser_in_outputs.png]]
#+LATEX: \captionof{figure}{In- and Output for Parser}\label{fig:in-and-output-for-parser}
#+LATEX: }
** Pare Tree
A parse tree a.k.a. Concrete Syntax Tree is a derivation of the syntax rules as tree.
A Concrete Syntax Tree contains everything, even the not required parentheses.
The order of the evaluation is implicitly given through the data structure.

The [[id:9d516b01-0fd4-416d-9bb8-8decd793fc82][Abstract Syntax Tree]] is a more minimal version of the Concrete Syntax Tree.

#+LATEX: {
[[file:img/concrete_syntax_tree.png]]
#+LATEX: \captionof{figure}{Concrete Syntax Tree}\label{fig:concrete-syntax-tree}
#+LATEX: }

** AST
The Abstract Syntax Tree is a possible output from the [[id:a027dc52-4c4c-4d7f-b139-ed4b28614150][Parser]].
The Abstract Syntax Tree contains only the required information.
For example parentheses are not stored for a [[id:d4e5f169-81a7-4ce9-a0ff-e85399b1294a][Java]] AST.

** Top-Down Parsing
A Top-Down [[id:a027dc52-4c4c-4d7f-b139-ed4b28614150][Parser]] will start with the Start Symbol form the [[id:9e64bd75-d1fc-40ef-80d2-fa8d70bc041b][EBNF]] and performs production on the input.
The Top-Down parser will expand from the start symbol to the terminal symbol.

#+LATEX: {
[[file:img/top_down_parsing.png]]
#+LATEX: \captionof{figure}{Top-Down Parsing}\label{fig:top-down-parsing}
#+LATEX: }

** Bottom-Up Parsing
A Bottom-Up parser starts with the input stream (tokens).
As soon as the parser can match the input on a rule it will reduce it.

#+LATEX: {
[[file:img/bottom_up_parsing.png]]
#+LATEX: \captionof{figure}{Bottom-Up Parsing}\label{fig:bottom-up-parsing}
#+LATEX: }
** Recursive Descent Parser
A recursive descent parser is a Top-Down Parser. 
For each non-terminal symbol in a [[id:9e64bd75-d1fc-40ef-80d2-fa8d70bc041b][EBNF]] you will implement a function.
During parsing, it will call the function =parseExpression= which will call =parseTerm=.
The =parseTerm= function will call again =parseExpression= (recursion).

The Recursive Descent Parser is a Push-Down Automata which uses the call stack as its stack.

#+begin_example
Expression = Term { ( "+" | "-" ) Term }.
Term = Number | "(" Expression ")".
#+end_example

#+begin_src java
  void parseExpression() {
      parseTerm();
      while (is(Tag.PLUS) || is(Tag.MINUS)) {
	  next();
	  parseTerm();
      }
  }

  void parseTerm() {
      // ...
      parseExpression();
      // ...
  }
#+end_src
** Parser Categories
- first letter
  - L :: read from left to right
  - R :: read from right to left
- second letter
  - L :: Left-most expansion (top-down parser)
  - R :: right-most reduction (bottom-up parser)
- number in parentheses
  - number of symbols look ahead

#+LATEX: {
[[file:img/parser_notation.png]]
#+LATEX: \captionof{figure}{Parser notation}\label{fig:parser-notation}
#+LATEX: }
* Semantic Checker
** Semantic Checker
A Semantic Checker takes the syntax tree ([[id:9d516b01-0fd4-416d-9bb8-8decd793fc82][AST]] / [[id:fc66c6d6-33a4-4e4a-9d73-8e44e19f0516][Concrete Syntax Tree]]) from the [[id:a027dc52-4c4c-4d7f-b139-ed4b28614150][Parser]] and returns the syntax tree with a [[id:4c8f2b6a-f6e3-4b93-b682-c9cea4e9b097][Symbol Table]].
The semantic checker performs context-sensitive checks as:
- declaration :: is every identifier uniquely declared
- types :: are the type rules satisfied
- methoden calls :: are arguments and parameters compatible
- and more (no cyclic inheritance, single main, ...)


To perform this checks the checker requires all declarations (variables, methods, classes) and all types (predefined, user defined, array, type polymorphism).
** Symbol Table
The Symbol Table is a data structure produced by the [[id:6bf110f5-151c-41bd-bd8d-95d9ff810b7d][Semantic Checker]].
The Symbol table is used to manage all declaration in a program source code.

The symbol table contains a mapping from /Symbol/ to the /AST/ node (=node = symbolTable.getDeclarationNode(symbol);=)


#+LATEX: {
[[file:img/symbol_table_example.png]]
#+LATEX: \captionof{figure}{Example Structure for a Symbol Table}\label{fig:example-structure-for-a-symbol-table}
#+LATEX: }

** Symbol Table Construction
The Semantic Checker performs the following steps:
1. construct the symbol table
   - traverse AST
   - start with global scope
   - insert symbol in parent scope
   - do not resolve type name and designator yet
   - do not forget Built-in values / types

2. resolve all types in table
   - set the type in symbol
   - see [[autoref:fig:resolve-types-in-symbol]]
3. resolve all declarations in [[id:9d516b01-0fd4-416d-9bb8-8decd793fc82][AST]]
   - see [[autoref:fig:resolve-declaration-in-ast]]
4. resolve types in AST
   - set type for each expression
   - see [[autoref:fig:resolve-types-in-ast]]


#+LATEX: {
[[file:img/resolve_types_in_sybols.png]]
#+LATEX: \captionof{figure}{Resolve Types in Symbol}\label{fig:resolve-types-in-symbol}
#+LATEX: }

#+LATEX: {
[[file:img/resolve_declaration_in_ast.png]]
#+LATEX: \captionof{figure}{Resolve Declaration in AST}\label{fig:resolve-declaration-in-ast}
#+LATEX: }

#+LATEX: {
[[file:img/resolve_types_in_ast.png]]
#+LATEX: \captionof{figure}{Resolve Types in AST}\label{fig:resolve-types-in-ast}
#+LATEX: }
* Code Generation
** Code Generator
The Code Generator takes the [[id:4c8f2b6a-f6e3-4b93-b682-c9cea4e9b097][Symbol Table]] and the [[id:9d516b01-0fd4-416d-9bb8-8decd793fc82][AST]] as input and generates machine code (Intel, ARM, JVM, ...).
Normally you want to separate the Code Generator from the [[id:a027dc52-4c4c-4d7f-b139-ed4b28614150][Parser]], [[id:6bf110f5-151c-41bd-bd8d-95d9ff810b7d][Semantic Checker]].
If you do so, you can use the same backend to produce code for various different platforms.

#+LATEX: {
[[file:img/backend_ast_frontend.png]]
#+LATEX: \captionof{figure}{Seperate Backend / Frontend}\label{fig:seperate-backend-frontend}
#+LATEX: }
** Evaluation Stack
In a VM (e.g. JVM - [[id:d4e5f169-81a7-4ce9-a0ff-e85399b1294a][Java]]) you may use an evaluation stack to run interpret your instructions.
The evaluation stack is an alternative to using registers (what you normally do on real processors).
For example the =imul= instruction will:
1. pop y
2. pop x
3. perform z = x * y
4. push z


Each function call has its own evaluation stack and should be empty at the beginning and at the end.


#+LATEX: {
[[file:img/eval_stack_imul.png]]
#+LATEX: \captionof{figure}{Example Evaluation Stack}\label{fig:example-evaluation-stack}
#+LATEX: }

* Virtual Machine
** VM
A VM is a piece of software which emulates a processor.
This processor can have a custom instruction set.

The benefit of a VM is:
- multiplatform
- multilanguage (multiple programming languages)
- security


#+LATEX: {
[[file:img/vm_example.png]]
#+LATEX: \captionof{figure}{VM Example}\label{fig:vm-example}
#+LATEX: }

** Loader
The loader
- reads the application binary into memory
- allocate memory
- defines memory layout
- perform address relocation
- starts program execution

** Descriptor
A descriptor contains the runtime information for types & methods:
- types: classes, arrays or base type
- classes: field types
- methods: parameter and local types, return value, byte code

** Ancestor Table
The ancestor table is used to check if a class inherits from another in constant time.
In [[autoref:fig:ancestor-table-example]] you see an example how such an ancestor table is constructed.
The last element in the table for each class is the reference to itself.
However, this only works for *Single-Inheritance*.

In languages like [[id:d4e5f169-81a7-4ce9-a0ff-e85399b1294a][Java]] you don't have to allocate memory for the =object= class, because every class inherits from it (in the ancestor table this would be -1).

#+LATEX: {
[[file:img/ancestor_table.png]]
#+LATEX: \captionof{figure}{Ancestor Table Example}\label{fig:ancestor-table-example}
#+LATEX: }

** Type Cast / Checking
You have a target type (you want to cast into this type) and a source type (you want to cast from this type).

1. Get the ancestor level from the target type (for example 0). Call the level ~x~
2. Check in the ancestor table of the source type if the ancestor at level ~x~ is the same as the target class


#+begin_src java
  private boolean typeTest(Pointer instance, ClassDescriptor targetType) {
      ClassDescriptor sourceType = heap.getDescriptor(instance);
      if (sourceType == targetType) {
	  return true;
      }
      var level = targetClass.getAncestorLevel();

      if (sourceClass.getAncestorTable().length > level) {
	  return sourceClass.getAncestorTable()[level] == targetClass;
      } else {
	  return false;
      }
  }
#+end_src

** Virtual Table
Each type has a Virtual Method Table (vtable).
The vtable describes which virtual method should be called when dynamic dispatch is performed. 
In a [[id:f6fab7f1-f45f-4dff-beb3-50f32cae922b][VM]] the vtable often points not directly to the function but to the method descriptor.

However, this only works with *Single-Inheritance*.

#+LATEX: {
[[file:img/vtable_example.png]]
#+LATEX: \captionof{figure}{VTable example}\label{fig:vtable-example}
#+LATEX: }
* GC
** Dangling Pointer
A dangling pointer is a pointer which points to a location in memory which is already freed.
Reading / Writing to such a pointer can / will cause problems (segfault).

** Memory Leak
We talk about memory leak if an object is not accessible anymore from the application but are not getting cleaned up.
This is not as dangerous as a [[id:b830d1e9-c178-4e43-b031-95c025ecab0c][Dangling Pointer]], but it will fill your memory with useless data.

** GC
The Garbage Collector is a piece of software which automatically cleans unused memory.
The usage of a Garbage Collector is memory safe.
That means:
- no [[id:b830d1e9-c178-4e43-b031-95c025ecab0c][Dangling Pointer]]
- no [[id:757392e4-dcce-41b4-adea-b1127ce74f5e][Memory Leak]]
- an overall simplification for programming

** Reference Counting
Reference counting is a mechanism to store the number of pointers to a reference.
Every time a pointer starts to point to a resource the counter is incremented.
As soon as one pointer does not point anymore to this location the counter is decremented.

** RC Cyclic Graph
As soon as you get a cyclic object structure the counter for each object can not reach zero.
Therefore, the Garbage Collector will never recognize it as garbage and a [[id:757392e4-dcce-41b4-adea-b1127ce74f5e][Memory Leak]] occurs.


#+LATEX: {
[[file:img/cyclic_object_structure.png]]
#+LATEX: \captionof{figure}{Cyclic Object Structure}\label{fig:cyclic-object-structure}
#+LATEX: }

** Mark & Sweep
The Mark And Sweep Algorithm can be used to inside a Garbage Collector to find and free garbage.
In the first phase (mark phase) you perform a [[id:c6045230-1e93-4222-9f2d-d5e2946eab25][DFS]] staring from a root set.
Every time you visit a node you mark it.

The root set consists of:
- all references in static variables
- all references in all activation frames on the call stack
- all references in the registers

In the second phase (sweep phase) you traverse the whole Heap linearly.
If the node is marked you clear the mark and look at the next node.
If the node is not marked you will free it.


#+LATEX: {
[[file:img/heap_graph.png]]
#+LATEX: \captionof{figure}{Heap Graph of Nodes}\label{fig:heap-graph-of-nodes}
#+LATEX: }

** GC Execution
The Garbage Collector is executed at the latest when the Heap is full.
However, most common Garbage Collector are executed earlier.

Depending on the implementation the application can not run, while the Garbage Collector is working (Stop & Go).


#+LATEX: {
[[file:img/stop_and_go_gc.png]]
#+LATEX: \captionof{figure}{Stop & Go GC}\label{fig:stop-go-gc}
#+LATEX: }

** Finalizer
A Finalizer is a method which is run just before the objects get deleted.
These functions are normally called by the [[id:c79b363f-ff8a-458a-9f8e-3b18dfd72a57][GC]] but not during normal GC phases.

Finalizers are executed in an arbitrary order and time.
They run concurrent to the main program.
In Java the finalizer is *NOT* executed again after [[id:5eaea609-1219-4eae-be70-c40fcabc8349][Resurrection]].

#+begin_src java
  class Block {
      @Override
      protected void finalize() {
	  //...
      }
  }
#+end_src
** Finalizer Execution: Time
A finalizer is called from the [[id:c79b363f-ff8a-458a-9f8e-3b18dfd72a57][GC]] but not during the normal GC phases. 
Reasons for this are:
- finalizer might run forever => would block GC
- finalizer migth create new objects => corrupts GC
- programming errors in finalizer => GC might crash
- finalizer can revive objects => [[id:5eaea609-1219-4eae-be70-c40fcabc8349][Resurrection]]
** Finalizer Execution: How
The [[id:c79b363f-ff8a-458a-9f8e-3b18dfd72a57][GC]] has a *finalizer set* where all finalizers are registered.
In the *pending queue* are all finalizers which are not executed yet but has to.
*Attention*: as soon as you insert the finalizer into the pending queue the object the revived ([[id:5eaea609-1219-4eae-be70-c40fcabc8349][Resurrection]]).
An additional GC phase is required.


#+LATEX: {
[[file:img/pending_queue.png]]
#+LATEX: \captionof{figure}{Pending Queue}\label{fig:pending-queue}
#+LATEX: }
** Resurrection
A finalizer can revive objects (no garbage anymore).
This can happen not only for the current object but also for other objects.


#+LATEX: {
[[file:img/resurrection_example.png]]
#+LATEX: \captionof{figure}{Resurrection Example}\label{fig:resurrection-example}
#+LATEX: }
** Weak References
A Weak Reference are references which are not counted for Reference Counting.
In a [[id:c79b363f-ff8a-458a-9f8e-3b18dfd72a57][GC]] context this can be used for object caches.
Weak References must be set to =NULL= after the target object is freed ([[id:b830d1e9-c178-4e43-b031-95c025ecab0c][Dangling Pointer]]).

#+LATEX: {
[[file:img/weak_reference.png]]
#+LATEX: \captionof{figure}{Weak Reference Example}\label{fig:weak-reference-example}
#+LATEX: }

** Compacting GC
A compacting GC always allocates at the end of the Heap right after the last object.
During the GC Phases (here mark and copy) the marked objects (non-garbage objects) are copied to the beginning.


#+LATEX: {
[[file:img/compacting_gc.png]]
#+LATEX: \captionof{figure}{Compacting GC Example}\label{fig:compacting-gc-example}
#+LATEX: }
** Incremental GC
An incremental GC runs *quasi parallel* to the mutator.
The main execution is interrupted only a very short time (No [[id:1df18b6b-6816-4921-8532-036b754b6624][Stop & Go GC]]).


#+LATEX: {
[[file:img/incremental_gc.png]]
#+LATEX: \captionof{figure}{Incremental GC}\label{fig:incremental-gc}
#+LATEX: }

** Generational GC
A Generational GC is an incremental GC because it often does not clean up the whole heap but in generations.
For example, you have 3 generations as describe in [[autoref:tbl:example-for-generations]]

#+LATEX: {
| age    | generation | GC frequency | GC break |
|--------+------------+--------------+----------|
| new    | G0         | high         | short    |
| middle | G1         | middle       | middle   |
| old    | G2         | low          | long     |
#+LATEX: \captionof{table}{Example for generations}\label{tbl:example-for-generations}
#+LATEX: }

If you want to clean up /Gn/ you must extend your root set with all references from the latter generations which are pointing into /Gn/.
If you want to clean up /Gn/ you also have to clean up all previous generations.

#+LATEX: {
[[file:img/gc_of_g0.png]]
#+LATEX: \captionof{figure}{GC of G0}\label{fig:gc-of-g0}
#+LATEX: }

** Partitioned GC
A Partitioned GC is an incremental GC.
The Heap is partitioned.
The GC will focus on the partition with the most garbage.
The non-garbage objects are moved to a new partition before cleaning up the partition (see [[autoref:fig:partitioned-gc]]).


#+LATEX: {
[[file:img/partitioned_gc.png]]
#+LATEX: \captionof{figure}{Partitioned GC}\label{fig:partitioned-gc}
#+LATEX: }
* JIT
** JIT
A Just-in-Time compiler is used in interpreted languages to compile the intermediate code (e.g. java byte code) to native machine code.
This process speeds up the execution.
Because JIT compiling takes time you normally don't want to compile everything but only so-called *Hot Spots*.
Hot Spots are code sections which are executed more often.

** Registers
In the original registers you have to specify another name.
For detail information see [[autoref:fig:register-sub-access]].


#+LATEX: {
[[file:img/register_subaccess.png]]
#+LATEX: \captionof{figure}{Register sub access}\label{fig:register-sub-access}
#+LATEX: }
** Registers Management
The JIT Compiler keeps track of used registers in a [[id:9b107c73-f9af-4c32-a9ea-de91614d6d1a][Stack]] (similar to the [[id:0777c9ea-94ba-44df-8fdf-ccaea0fb36cf][Evaluation Stack]]).
The stack is updated for each translated byte code instruction.
** Register Clobbering
For example the /IDVI/ assembler instruction overrides the values in /RAX/ and /RDX/ as side effects.
This is called register clobbering.
Therefore, you have to save thoses values before you executed /IDIV/ (Register Relocation).

* Code Optimization
** Optimizer
The Optimizer is a part of the compiler.
The optimizer transforms an Intermediate Representation or machine code with the goal to generate a more efficient version.
This is often done using multiple steps.
Sometimes the same step is performed multiple times.
** Strategies
- arithmetic optimization
  - replace =*, /, %= with bit operations when possible
- algebraic simplification
  - replace const expression with const value
  - =expr / 1 -> expr=
  - =expr * 0 -> ldc 0=
  - =1 + 3 -> ldc 4=
- loop invariant code
  - calculate unchanged expressions during loop outside the loop (see [[autoref:lst:loop-invariant]] and [[autoref:lst:optimizied-loop]])
- common sub-expressions
  - calculate same sub-expression only once (var are not allowed to change in between) (see [[autoref:lst:common-sub-expression]])
- dead code
  - eliminate dead code (May has to be performed multiple times to eliminate all dead code)
- redundant read and write
  - this may lead to dead code elimination
- constant propagation / constant folding
  - some variables are constant even after some long operations
  - load constant, may lead to dead code (in [[autoref:lst:constant-propagation]] =ldc 3= has to be performed)
- partial redundancy
  - some expressions are evaluated one or more times (depending on the branching)
  - evaluate only once


#+CAPTION: Loop invariant
#+NAME: lst:loop-invariant
#+begin_src java
  while (x < N * M) { // N * M does never change during loop
    k = y * M;        // k does not change during loop
    x = x + k;
  }
#+end_src


#+CAPTION: Optimizied Loop
#+NAME: lst:optimizied-loop
#+begin_src java
  k = y * M;
  temp = N * M;
  while (x < temp) {
    x = x + k;
  }
#+end_src


#+CAPTION: Common sub-expression
#+NAME: lst:common-sub-expression
#+begin_src java
  x = a * b + c;
  // operations, but no changes on a and b
  y = a * b + d;

  temp = a * b;
  x = temp + c;
  // operations, but no changes on a and b
  y = temp + d;
#+end_src

#+CAPTION: Constant Propagation
#+NAME: lst:constant-propagation
#+begin_src java
  a = 1;
  if (...) {
      a = a + 1; // a = 2
      b = a;     // b = 2
  } else {
      b = 2;
  }
  c = b + 1;    // c = 2
#+end_src
** SSA
Static Single Assignment is a method to detect optimization possibilities.
In SSA each variable is assigned only once.
If a variable is assigned multiple times you rename it.
Assignments in different branches also get different indexes.
To find the right index at the [[id:63f650ca-9ada-4f28-92c3-0a1a5f785e5c][Phi Function]] is used.

#+CAPTION: SSA with branches
#+NAME: lst:ssa-with-branches
#+begin_src java
  if (...) {
      x_1 = 1;
  } else {
      x_2 = 2;
  }
  y_1 = x_?;
#+end_src
** Phi Function
The Phi Function is used to check which =x= should be used during SSA after branching.
In [[autoref:lst:phi-function]] the phi function performs the following:
- return =x_1= if the =if= branch was taken
- return =x_2= if the =else= branch was taken

#+CAPTION: Phi-Function
#+NAME: lst:phi-function
#+begin_src java
  if (...) {
      x_1 = 1;
  } else {
      x_2 = 2;
  }
  y_1 = phi(x_1, x_2);
#+end_src
** SSA and Common Subexpression
Using SSA you can detect multiple assignments to the same variable (or the absence of them).
Thanks to this, you can detect common sub-expressions.

#+CAPTION: Common sub-expressions in SSA
#+NAME: lst:common-sub-expressions-in-ssa
#+begin_src java
  x_1 = a_1 * b_1 + c_1;
  // some operations
  y_1 = a_1 * b_1 + d_1; // no changes on a and b otherwise index would be incremented
  // some operations
  z = a_1 * b_2 + d_3;
#+end_src
** SSA and Dead Code
Thanks to SSA you can detect that =x_1= is never read.
Therefore, you don't have to generate code for this expression.

#+CAPTION: Dead code detection
#+NAME: lst:dead-code-detection
#+begin_src java
  x_1 = 1;
  x_2 = 2;
  y_1 = x_2 + 1;
  writeInt(y_1);
#+end_src
** Peephole Optimization
Peephole Optimization is used only on a small set of instruction.
This set of instruction is changed using a Sliding Window.
The Optimizations are performed on the sliding window.


#+LATEX: {
[[file:img/peephole_optimization.png]]
#+LATEX: \captionof{figure}{Peephole Optimization Example}\label{fig:peephole-optimization-example}
#+LATEX: }

* END
#+LATEX: \end{multicols}
